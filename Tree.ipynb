{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca96bda7-8e69-4d1a-815a-c8e9da137d7a",
   "metadata": {},
   "source": [
    "step 1:  Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b37b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3dd49-ba60-48d0-a331-4263b20afc43",
   "metadata": {},
   "source": [
    "step 2: Define a class to represent a node in the decision tree\n",
    "\n",
    "    Constructor for a tree node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723bece4-45a2-48a4-892b-123cae5a932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, var_red=None, value=None):\n",
    "        '''         \n",
    "        Parameters:\n",
    "        feature_index (int): Index of the feature used for splitting.\n",
    "        threshold (float): Threshold value for splitting.\n",
    "        left (Node): Left child node.\n",
    "        right (Node): Right child node.\n",
    "        var_red (float): Variance reduction achieved by the split.\n",
    "        value (float): Value of the leaf node (for predictions).\n",
    "        '''\n",
    "\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.var_red = var_red\n",
    "\n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e94f2-0551-467b-aeda-9f4296d13d00",
   "metadata": {},
   "source": [
    "step 3: Define a class for a custom decision tree regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85452814-7000-43ae-8bb3-97473c9de496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split=2, ):\n",
    "        \"\"\"\n",
    "        Constructor for a custom decision tree regressor.\n",
    "\n",
    "        Parameters:\n",
    "        min_samples_split (int): Minimum number of samples required to split.\n",
    "        \"\"\"\n",
    "        self.root = None  # Root node of the tree\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        \"\"\"\n",
    "        Recursive function to build the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        dataset (ndarray): Training data including features and target values.\n",
    "        curr_depth (int): Current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        Node: A node representing the decision tree structure.\n",
    "        \"\"\"\n",
    "        X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "\n",
    "        # Check stopping conditions\n",
    "        if num_samples >= self.min_samples_split:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split and best_split.get(\"var_red\", 0) > 0:\n",
    "                # Build left and right subtrees recursively\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], left_subtree, right_subtree, best_split[\"var_red\"])\n",
    "\n",
    "        # Compute and return a leaf node if stopping conditions are met\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\"\n",
    "        Find the best split for the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        dataset (ndarray): Training data including features and target values.\n",
    "        num_samples (int): Number of samples in the dataset.\n",
    "        num_features (int): Number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        dict: Information about the best split.\n",
    "        \"\"\"\n",
    "        best_split = {}\n",
    "        max_var_red = -float(\"inf\")\n",
    "\n",
    "        # Iterate over all features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            # Try each possible threshold\n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n",
    "                    if curr_var_red > max_var_red:\n",
    "                        best_split = {\n",
    "                            \"feature_index\": feature_index,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"dataset_left\": dataset_left,\n",
    "                            \"dataset_right\": dataset_right,\n",
    "                            \"var_red\": curr_var_red\n",
    "                        }\n",
    "                        max_var_red = curr_var_red\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        \"\"\"\n",
    "        Split the dataset based on a feature and a threshold.\n",
    "\n",
    "        Parameters:\n",
    "        dataset (ndarray): Training data.\n",
    "        feature_index (int): Index of the feature to split on.\n",
    "        threshold (float): Threshold value for the split.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Left and right splits of the dataset.\n",
    "        \"\"\"\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def variance_reduction(self, parent, l_child, r_child):\n",
    "        \"\"\"\n",
    "        Calculate variance reduction achieved by a split.\n",
    "\n",
    "        Parameters:\n",
    "        parent (ndarray): Target values of the parent node.\n",
    "        l_child (ndarray): Target values of the left child.\n",
    "        r_child (ndarray): Target values of the right child.\n",
    "\n",
    "        Returns:\n",
    "        float: Variance reduction.\n",
    "        \"\"\"\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
    "        return reduction\n",
    "\n",
    "    def calculate_leaf_value(self, Y):\n",
    "        \"\"\"\n",
    "        Calculate the value of a leaf node.\n",
    "\n",
    "        Parameters:\n",
    "        Y (ndarray): Target values of the samples in the leaf.\n",
    "\n",
    "        Returns:\n",
    "        float: Mean of the target values.\n",
    "        \"\"\"\n",
    "        return np.mean(Y)\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\"\n",
    "        Print the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        tree (Node): The root node of the tree (default is None, which means the root of the current tree).\n",
    "        indent (str): Indentation for tree visualization.\n",
    "        \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(f\"X_{tree.feature_index} <= {tree.threshold}? {tree.var_red}\")\n",
    "            print(f\"{indent}left:\", end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(f\"{indent}right:\", end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix.\n",
    "        Y (ndarray): Target values.\n",
    "        \"\"\"\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def make_prediction(self, x, tree):\n",
    "        \"\"\"\n",
    "        Make a prediction for a single sample.\n",
    "\n",
    "        Parameters:\n",
    "        x (ndarray): Feature vector of the sample.\n",
    "        tree (Node): The root node of the tree.\n",
    "\n",
    "        Returns:\n",
    "        float: Predicted value.\n",
    "        \"\"\"\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for multiple samples.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "        list: Predicted values.\n",
    "        \"\"\"\n",
    "        return [self.make_prediction(x, self.root) for x in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413e359-ba04-491e-84f5-1a52e3de7576",
   "metadata": {},
   "source": [
    "step 4: Define a class for Ensemble Regression Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b43f021-1137-4c8f-bb59-53a55fa8c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleRegressionTree:\n",
    "    def __init__(self, n_trees=5, min_samples_split=10):\n",
    "        \"\"\"\n",
    "        Constructor for Ensemble Regression Tree.\n",
    "\n",
    "        Parameters:\n",
    "        n_trees (int): Number of trees in the ensemble.\n",
    "        min_samples_split (int): Minimum number of samples required to split a node.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []  # List to store individual trees\n",
    "        self.initial_prediction = None  # Initial prediction (mean of the target values)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the ensemble regression tree.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix.\n",
    "        y (ndarray): Target values.\n",
    "        \"\"\"\n",
    "        # Initialize the prediction as the mean of the target values\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(y.shape, self.initial_prediction)\n",
    "\n",
    "        for i in range(self.n_trees):\n",
    "            # Calculate residuals (difference between actual and predicted values)\n",
    "            residuals = y.ravel() - predictions.ravel()\n",
    "            # Train a new tree to predict the residuals\n",
    "            tree = MyDecisionTreeRegressor(min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X, residuals.reshape(-1, 1))  # Fit the tree with residuals\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Update predictions with the current tree's output\n",
    "            predictions += np.array(tree.predict(X)).reshape(-1, 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for new data using the trained ensemble.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "        ndarray: Predicted values.\n",
    "        \"\"\"\n",
    "        predictions = np.full((X.shape[0], 1), self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            predictions += np.array(tree.predict(X)).reshape(-1, 1)\n",
    "        return predictions.ravel()  # Return predictions as a 1D array\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"\n",
    "        Print the structure of all trees in the ensemble.\n",
    "        \"\"\"\n",
    "        print(f\"Initial prediction: {self.initial_prediction}\")\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            print(f\"\\nTree {i + 1}:\")\n",
    "            tree.print_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0c240-9ff4-4dc5-ae24-6badcd4805fc",
   "metadata": {},
   "source": [
    "step 5: find best msp condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c510193f-9cc3-46b6-8fc4-c9322f34c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def auto_tune_min_samples_split(X, y, n_trees, start_msp=18, step=2, max_msp=40, epsilon=1e-4, patience=3, mse_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Automatically tune the best `min_samples_split` for an ensemble regression tree .\n",
    "    \"\"\"\n",
    "    best_msp = None\n",
    "    best_mse = float('inf')\n",
    "    previous_mse = float('inf')\n",
    "    no_improvement_steps = 0\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    tried_msp = []  # Track all tried msp values for logging\n",
    "\n",
    "    msp = start_msp\n",
    "\n",
    "    while msp <= max_msp:\n",
    "        mse_list = []\n",
    "\n",
    "        # Perform K-Fold cross-validation\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Train the ensemble with current min_samples_split\n",
    "            ensemble = EnsembleRegressionTree(n_trees=n_trees, min_samples_split=msp)\n",
    "            ensemble.fit(X_train, y_train)\n",
    "\n",
    "            # Validate the model\n",
    "            y_pred = ensemble.predict(X_val)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mse_list.append(mse)\n",
    "\n",
    "        # Calculate average MSE for current `min_samples_split`\n",
    "        avg_mse = np.mean(mse_list)\n",
    "        tried_msp.append((msp, avg_mse))\n",
    "        print(f\"min_samples_split={msp}, Avg MSE={avg_mse}, Fold MSEs={mse_list}\")\n",
    "\n",
    "        # Update the best MSE and parameters if improved\n",
    "        if avg_mse < best_mse:\n",
    "            best_mse = avg_mse\n",
    "            best_msp = msp\n",
    "            no_improvement_steps = 0  # Reset patience counter\n",
    "        else:\n",
    "            no_improvement_steps += 1\n",
    "\n",
    "        # Early stopping based on epsilon\n",
    "        if avg_mse < mse_threshold and abs(previous_mse - avg_mse) < epsilon:\n",
    "            no_improvement_steps += 1\n",
    "\n",
    "        if no_improvement_steps >= patience:\n",
    "            print(\"No significant improvement for\", patience, \"steps. Final stopping.\")\n",
    "            break\n",
    "\n",
    "        previous_mse = avg_mse\n",
    "        msp += step  # Increment `min_samples_split`\n",
    "\n",
    "    # Log all tried values\n",
    "    print(\"\\nTried min_samples_split values and corresponding MSEs:\")\n",
    "    for msp_val, mse_val in tried_msp:\n",
    "        print(f\"min_samples_split={msp_val}, Avg MSE={mse_val}\")\n",
    "\n",
    "    return best_msp, best_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121fbd0f-3e95-468c-9ae8-fd773fe2f9e1",
   "metadata": {},
   "source": [
    "step 6: Main program for running the custom and ensemble regression tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d403866b-86d5-46fd-a210-d5be9938473a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Auto-Tuning for Ensemble Regression Tree =====\n",
      "min_samples_split=2, Avg MSE=0.5712439576797841, Fold MSEs=[0.5373522927234916, 0.6340690167165753, 0.5039318480466096, 0.6076035389908367, 0.5732630919214068]\n",
      "min_samples_split=4, Avg MSE=0.5598420212053671, Fold MSEs=[0.5148538542719803, 0.6240564528256615, 0.4974698741085892, 0.5905757374499255, 0.572254187370679]\n",
      "min_samples_split=6, Avg MSE=0.5556263242652387, Fold MSEs=[0.5184067168330802, 0.6126470473009001, 0.4886126253529694, 0.5882366059623276, 0.5702286258769163]\n",
      "min_samples_split=8, Avg MSE=0.5546729362825766, Fold MSEs=[0.5239644551410465, 0.6147590236149829, 0.48718644047600723, 0.5747438257773182, 0.572710936403528]\n",
      "min_samples_split=10, Avg MSE=0.5519193441553465, Fold MSEs=[0.5265777540915385, 0.6164172105632053, 0.48758164881840754, 0.561349111842056, 0.5676709954615256]\n",
      "min_samples_split=12, Avg MSE=0.5523972298701171, Fold MSEs=[0.5328375375891711, 0.6176423314937809, 0.48555133748025125, 0.559050054635783, 0.5669048881515988]\n",
      "min_samples_split=14, Avg MSE=0.552988323540075, Fold MSEs=[0.5285918952121912, 0.6116246505624182, 0.49376002807337216, 0.5598408688023767, 0.5711241750500168]\n",
      "min_samples_split=16, Avg MSE=0.5545082801768982, Fold MSEs=[0.5291221719072847, 0.6194487254558654, 0.48994939458023723, 0.5645097704039123, 0.5695113385371917]\n",
      "No significant improvement for 3 steps. Final stopping.\n",
      "\n",
      "Tried min_samples_split values and corresponding MSEs:\n",
      "min_samples_split=2, Avg MSE=0.5712439576797841\n",
      "min_samples_split=4, Avg MSE=0.5598420212053671\n",
      "min_samples_split=6, Avg MSE=0.5556263242652387\n",
      "min_samples_split=8, Avg MSE=0.5546729362825766\n",
      "min_samples_split=10, Avg MSE=0.5519193441553465\n",
      "min_samples_split=12, Avg MSE=0.5523972298701171\n",
      "min_samples_split=14, Avg MSE=0.552988323540075\n",
      "min_samples_split=16, Avg MSE=0.5545082801768982\n",
      "Best min_samples_split for ensemble regression tree: 10, Best MSE: 0.5519193441553465\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train = pd.read_csv('train_x.csv')\n",
    "    trainGT = pd.read_csv('train_y.csv')\n",
    "    test = pd.read_csv('test_x.csv')\n",
    "    testGT = pd.read_csv('test_y.csv')\n",
    "\n",
    "    # Extract features and targets from the datasets\n",
    "    X_train = train.iloc[:, :-1].values\n",
    "    X_test = test.iloc[:, :-1].values\n",
    "    y_train = trainGT.iloc[:, -1].values.reshape(-1, 1)  # Reshape to 2D\n",
    "    y_test = testGT.iloc[:, -1].values.reshape(-1, 1)  # Reshape to 2D\n",
    "\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)  # Fit on training data\n",
    "    X_test = scaler.transform(X_test)  # Apply the same transformation to test data\n",
    "\n",
    "\n",
    "    # Automatically tune min_samples_split for ensemble regression tree\n",
    "    print(\"\\n===== Auto-Tuning for Ensemble Regression Tree =====\")\n",
    "    best_msp_ensemble, best_ensemble_mse = auto_tune_min_samples_split(X_train, y_train, n_trees=5,start_msp=2)\n",
    "    print(f\"Best min_samples_split for ensemble regression tree: {best_msp_ensemble}, Best MSE: {best_ensemble_mse}\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "603543a0-7bc8-4b26-913c-57ae753c2691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train reshaped: (5196, 1)\n",
      "\n",
      "===== Training Models with Tuned Parameters =====\n",
      "\n",
      "===== Training Ensemble Regression Tree with Fixed Parameters =====\n",
      "\n",
      "===== Model Performance =====\n",
      "MSE (sklearn): 0.6674364896073903\n",
      "MSE (custom): 0.6674364896073903\n",
      "MSE (ensemble with parameters {'n_trees': 5, 'msp': 10}): 0.7103643014814119\n"
     ]
    }
   ],
   "source": [
    "    # Load and preprocess data\n",
    "    train = pd.read_csv('train_x.csv')\n",
    "    trainGT = pd.read_csv('train_y.csv')\n",
    "    test = pd.read_csv('test_x.csv')\n",
    "    testGT = pd.read_csv('test_y.csv')\n",
    "\n",
    "    # Extract features and targets from the datasets\n",
    "    X_train = train.iloc[:, :-1].values\n",
    "    X_test = test.iloc[:, :-1].values\n",
    "    y_train = trainGT.iloc[:, -1].values.reshape(-1, 1)  # Reshape to 2D\n",
    "    y_test = testGT.iloc[:, -1].values.reshape(-1, 1)  # Reshape to 2D\n",
    "\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "#     print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "    # Check and reshape y_train to ensure compatibility\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "    print(f\"y_train reshaped: {y_train.shape}\")\n",
    "\n",
    "    # Train and compare sklearn decision tree and custom decision tree with auto-tuned min_samples_split\n",
    "    print(\"\\n===== Training Models with Tuned Parameters =====\")\n",
    "    sklearn_tree = DecisionTreeRegressor()\n",
    "    sklearn_tree.fit(X_train, y_train)\n",
    "\n",
    "    my_tree = MyDecisionTreeRegressor()\n",
    "    my_tree.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions for sklearn and custom decision trees\n",
    "    y_pred_sklearn = sklearn_tree.predict(X_test)\n",
    "    y_pred_custom = my_tree.predict(X_test)\n",
    "\n",
    "    # Train ensemble regression tree with specific parameters: {'n_trees': 5, 'msp': 10}\n",
    "    print(\"\\n===== Training Ensemble Regression Tree with Fixed Parameters =====\")\n",
    "    ensemble = EnsembleRegressionTree(n_trees=5, min_samples_split=best_msp_ensemble)\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    y_pred_ensemble = ensemble.predict(X_test)\n",
    "\n",
    "    # Print predictions and performance comparison\n",
    "    print(\"\\n===== Model Performance =====\")\n",
    "    print(\"MSE (sklearn):\", mean_squared_error(y_test, y_pred_sklearn))\n",
    "    print(\"MSE (custom):\", mean_squared_error(y_test, y_pred_custom))\n",
    "\n",
    "    print(\"MSE (ensemble with parameters {'n_trees': 5, 'msp': best msp}):\", \n",
    "          mean_squared_error(y_test, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b51e3-e2a2-4586-8058-70371e51a186",
   "metadata": {},
   "source": [
    "print tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "169bd6bd-92fa-484e-8582-68417794557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"-------my_tree structure-------\")\n",
    "    my_tree.print_tree()\n",
    "\n",
    "    # print(\"-------ensemble_tree structure-------\")\n",
    "    # ensemble.print_tree()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba38b80-dace-40c6-9073-9be6f6b0519d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
